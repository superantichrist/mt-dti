{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from src.finetune.dti_model import MbertPcnnModel\n",
    "import argparse\n",
    "import _pickle as cPickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--base_path'], dest='base_path', nargs=None, const=None, default='../../data', type=<class 'str'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training parser')\n",
    "parser.add_argument('--gpu_num', default=\"0\", choices=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"], type=str)\n",
    "parser.add_argument('--model_version', default=\"1\", choices=[\"1\", \"2\", \"3\", \"4\", \"11\", \"14\"], type=str)\n",
    "parser.add_argument('--batch_size', default=512, choices=[256, 512], type=int)\n",
    "parser.add_argument('--fold', default=0, choices=[0,1,2,3,4], type=int)\n",
    "parser.add_argument('--dataset_name', type=str, default=\"kiba\", choices=[\"davis\", \"kiba\"],\n",
    "                    help='dataset_name')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-4,\n",
    "                    help='learning_rate')\n",
    "parser.add_argument('--tpu_name', type=str, default=\"btpu\",\n",
    "                    help='tpu_name')\n",
    "parser.add_argument('--use_tpu', type=bool, default=False,\n",
    "                    help='use_tpu')\n",
    "parser.add_argument('--tpu_zone', type=str, default=\"us-central1-b\",\n",
    "                    help='tpu_zone')\n",
    "parser.add_argument('--num_tpu_cores', type=int, default=8,\n",
    "                    help='num_tpu_cores')\n",
    "parser.add_argument('--bert_config_file', type=str, default=\"../../config/m_bert_base_config.json\",\n",
    "                    help='bert_config_file')\n",
    "parser.add_argument('--init_checkpoint', type=str, default=\"../../data/pretrain/mbert_6500k/model.ckpt-6500000\",\n",
    "                    help='init_checkpoint')\n",
    "parser.add_argument('--k1', type=int, default=12, help='kernel_size1')\n",
    "parser.add_argument('--k2', type=int, default=12, help='kernel_size2')\n",
    "parser.add_argument('--k3', type=int, default=12, help='kernel_size3')\n",
    "\n",
    "parser.add_argument('--base_path', default=\"../../data\", type=str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_num\n",
    "\n",
    "i_trn = \"../../data/%s/tfrecord/fold%d.trn.tfrecord\" % (args.dataset_name, args.fold)\n",
    "i_dev= \"../../data/%s/tfrecord/fold%d.dev.tfrecord\" % (args.dataset_name, args.fold)\n",
    "i_tst= \"../../data/%s/tfrecord/fold%d.tst.tfrecord\" % (args.dataset_name, args.fold)\n",
    "output_dir = \"../../data/%s/mbert_cnn_v%s_lr%.4f_k%d_k%d_k%d_fold%d/\" % (args.dataset_name, args.model_version, args.learning_rate, args.k1, args.k2, args.k3, args.fold)\n",
    "best_model_dir_mse = \"../../data/%s/mbert_cnn_v%s_lr%.4f_k%d_k%d_k%d_fold%d/best_mse\" % (args.dataset_name, args.model_version, args.learning_rate, args.k1, args.k2, args.k3, args.fold)\n",
    "best_model_dir_ci = \"../../data/%s/mbert_cnn_v%s_lr%.4f_k%d_k%d_k%d_fold%d/best_ci\" % (args.dataset_name, args.model_version, args.learning_rate, args.k1, args.k2, args.k3, args.fold)\n",
    "\n",
    "if args.dataset_name==\"kiba\":\n",
    "    num_trn_example = 78835\n",
    "    batch_size = args.batch_size\n",
    "    # num_train_steps = 154000  # (78835/512)*1000 = 153974, 1000 epoch\n",
    "    num_train_steps = int(num_trn_example*1.0/batch_size*1000)  # (78835/512)*1000 = 153974, 1000 epoch\n",
    "    num_warmup_steps = num_train_steps//10\n",
    "    dev_batch_size = 1\n",
    "    dev_steps = 19709 # 19709/1=19709\n",
    "    save_checkpoints_steps = 150 # 78835/512 = 157.67\n",
    "\n",
    "\n",
    "elif args.dataset_name==\"davis\":\n",
    "    num_trn_example = 20035\n",
    "    batch_size = args.batch_size\n",
    "    # num_train_steps = 40000  # (20035/512)*1000 = 39130, 1000 epoch\n",
    "    num_train_steps = int(num_trn_example * 1.0 / batch_size * 1000)  # (78835/512)*1000 = 153974, 1000 epoch\n",
    "    num_warmup_steps = num_train_steps // 10\n",
    "    dev_batch_size = 5009\n",
    "    dev_steps = 1 # 5009/1=5009\n",
    "    save_checkpoints_steps = 40  # 20035/512 = 40.07\n",
    "\n",
    "elif args.dataset_name==\"metz\":\n",
    "    num_trn_example = 20035\n",
    "    batch_size = args.batch_size\n",
    "    num_train_steps = 12021  # (20035/500)*300 = 12, 300 epoch\n",
    "    num_warmup_steps = num_train_steps // 10\n",
    "    dev_batch_size = 5009\n",
    "    dev_steps = 1 # 5009/1=5009\n",
    "    save_checkpoints_steps = 158  # 20035/500 = 40.07\n",
    "else:\n",
    "    batch_size = None\n",
    "    num_train_steps = None\n",
    "    num_warmup_steps = None\n",
    "    dev_batch_size = None\n",
    "    dev_steps = None\n",
    "    save_checkpoints_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    del argv\n",
    "\n",
    "    # TODO: refactoring is required: seq_to_id.cpkl should be in one of the preprocessings\n",
    "    lookup_file_name = \"%s/%s/seq_to_id.cpkl\" % (args.base_path, args.dataset_name)\n",
    "    with open(lookup_file_name, 'rb') as handle:\n",
    "        (mseq_to_id, pseq_to_id) = cPickle.load(handle)\n",
    "\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    # init model class\n",
    "    model = MbertPcnnModel(batch_size, dev_batch_size, 100, 1000,\n",
    "                           args.bert_config_file, args.init_checkpoint,\n",
    "                           args.learning_rate, num_train_steps, num_warmup_steps, args.use_tpu,\n",
    "                           args.k1, args.k2, args.k3)\n",
    "\n",
    "    tpu_cluster_resolver = None\n",
    "    if args.use_tpu and args.tpu_name:\n",
    "        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "            args.tpu_name, zone=args.tpu_zone, project=None)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        session_config=config,\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        master=None,\n",
    "        model_dir=output_dir,\n",
    "        save_checkpoints_steps=save_checkpoints_steps,\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "            iterations_per_loop=save_checkpoints_steps,\n",
    "            num_shards=args.num_tpu_cores,\n",
    "            per_host_input_for_training=is_per_host))\n",
    "\n",
    "    model_fn = eval(\"model.model_fn_v%s\" % args.model_version)\n",
    "    # create classifier\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=False,\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        train_batch_size=batch_size,\n",
    "        eval_batch_size=dev_batch_size)\n",
    "\n",
    "    input_fn_tst = model.input_fn_builder([i_tst], is_training=False)\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    # 19708/4 = 4927\n",
    "    print(\"====================================== tst ==============================\")\n",
    "    results = estimator.predict(input_fn=input_fn_tst)\n",
    "    filename = \"%s/%s/mtdti.v%s.predictions.fold%d.txt\" % (args.base_path, args.dataset_name, args.model_version, args.fold)\n",
    "    print(filename)\n",
    "    with open(filename, 'wt') as handle:\n",
    "        # handle.write(\"chemid,pid,y_hat,y,smiles,fasta\\n\")\n",
    "        handle.write(\"chemid,pid,y_hat,y\\n\")\n",
    "        for idx, result in enumerate(results):\n",
    "            xd_str = ','.join(map(str, result['xd']))\n",
    "            xt_str = ','.join(map(str, result['xt']))\n",
    "\n",
    "            if xd_str in mseq_to_id:\n",
    "                smiles = mseq_to_id[xd_str][0]\n",
    "                chemid = mseq_to_id[xd_str][1]\n",
    "            else:\n",
    "                chemid = 0\n",
    "\n",
    "            if xt_str in pseq_to_id:\n",
    "                fasta = pseq_to_id[xt_str][0]\n",
    "                pid = pseq_to_id[xt_str][1]\n",
    "            else:\n",
    "                pid = 0\n",
    "\n",
    "            y_hat = result['predictions'][0]\n",
    "            y = result['gold'][0]\n",
    "\n",
    "            # oneline = \"%s,%s,%f,%f,%s,%s\\n\" % (chemid, pid, y_hat, y, smiles, fasta)\n",
    "            oneline = \"%s,%s,%f,%f\\n\" % (chemid, pid, y_hat, y)\n",
    "            handle.write(oneline)\n",
    "            # print(oneline)\n",
    "            if idx % 1000 == 0:\n",
    "                print(idx)\n",
    "\n",
    "    # print(idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<bound method MbertPcnnModel.model_fn_v1 of <src.finetune.dti_model.MbertPcnnModel object at 0x7f99553190f0>>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_evaluation_master': '', '_global_id_in_cluster': 0, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': None, '_keep_checkpoint_max': 5, '_task_id': 0, '_device_fn': None, '_protocol': None, '_save_checkpoints_steps': 150, '_experimental_distribute': None, '_task_type': 'worker', '_model_dir': '../../data/kiba/mbert_cnn_v1_lr0.0001_k12_k12_k12_fold0/', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9955319160>, '_master': '', '_service': None, '_num_worker_replicas': 1, '_log_step_count_steps': None, '_tf_random_seed': None, '_eval_distribute': None, '_tpu_config': TPUConfig(iterations_per_loop=150, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_train_distribute': None, '_save_summary_steps': 100, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.9\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_num_ps_replicas': 0, '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "====================================== tst ==============================\n",
      "../../data/kiba/mtdti.v1.predictions.fold0.txt\n",
      "WARNING:tensorflow:From ../../src/finetune/dti_model.py:525: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "INFO:tensorflow:*********************************** MbertPcnnModel V1 ***********************************\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../../data/kiba/mbert_cnn_v1_lr0.0001_k12_k12_k12_fold0/model.ckpt-153974\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "tf.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
